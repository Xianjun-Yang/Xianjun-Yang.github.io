---
---

@inproceedings{Wang2023TRACEAC,
  title={TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models},
  author={Xiao Wang and Yuansen Zhang and Tianze Chen and Songyang Gao and Senjie Jin and Xianjun Yang and Zhiheng Xi and Rui Zheng and Yicheng Zou and Tao Gui and Qi Zhang and Xuanjing Huang},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:263830425}
}

@inproceedings{Yang2023ShadowAT,
  title={Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models},
  author={Xianjun Yang and Xiao Wang and Qi Zhang and Linda Petzold and William Yang Wang and Xun Zhao and Dahua Lin},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:263620436}
}

@article{yang2023zero,
  title={Zero-Shot Detection of Machine-Generated Codes},
  author={Yang, Xianjun and Zhang, Kexun and Chen, Haifeng and Petzold, Linda and Wang, William Yang and Cheng, Wei},
  journal={arXiv preprint arXiv:2310.05103},
  year={2023}
}

@article{xiao2023large,
  title={Large Language Models Can Be Good Privacy Protection Learners},
  author={Xiao, Yijia and Jin, Yiqiao and Bai, Yushi and Wu, Yue and Yang, Xianjun and Luo, Xiao and Yu, Wenchao and Zhao, Xujiang and Liu, Yanchi and Chen, Haifeng and others},
  journal={arXiv preprint arXiv:2310.02469},
  year={2023}
}

@article{yang2023dna,
  title={DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text},
  author={Yang, Xianjun and Cheng, Wei and Petzold, Linda and Wang, William Yang and Chen, Haifeng},
  journal={arXiv preprint arXiv:2305.17359},
  year={2023}
}

@article{zhang2023enhancing,
  title={Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting},
  author={Zhang, Xinlu and Li, Shiyang and Yang, Xianjun and Tian, Chenxin and Qin, Yao and Petzold, Linda Ruth},
  journal={arXiv preprint arXiv:2305.12723},
  year={2023}
}

@article{lu2023llmscore,
  title={LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation},
  author={Lu, Yujie and Yang, Xianjun and Li, Xiujun and Wang, Xin Eric and Wang, William Yang},
  journal={arXiv preprint arXiv:2305.11116},
  year={2023}
}

@article{yang2023dynamic,
  title={Dynamic Prompting: A Unified Framework for Prompt Tuning},
  author={Yang, Xianjun and Cheng, Wei and Zhao, Xujiang and Petzold, Linda and Chen, Haifeng},
  journal={arXiv preprint arXiv:2303.02909},
  year={2023}
}

@article{yang2023exploring,
  title={Exploring the limits of chatgpt for query or aspect-based text summarization},
  author={Yang, Xianjun and Li, Yan and Zhang, Xinlu and Chen, Haifeng and Cheng, Wei},
  journal={arXiv preprint arXiv:2302.08081},
  year={2023}
}

@article{yang2023matkb,
  title={MatKB: Semantic Search for Polycrystalline Materials Synthesis Procedures},
  author={Yang, Xianjun and Wilson, Stephen and Petzold, Linda},
  journal={arXiv preprint arXiv:2302.05597},
  year={2023}
}

@article{zhang2023redi,
  title={ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval},
  author={Zhang, Kexun and Yang, Xianjun and Wang, William Yang and Li, Lei},
  journal={arXiv preprint arXiv:2302.02285},
  year={2023}
}

@inproceedings{yang-etal-2023-oasum,
    title = "{OAS}um: Large-Scale Open Domain Aspect-based Summarization",
    author = "Yang, Xianjun  and
      Song, Kaiqiang  and
      Cho, Sangwoo  and
      Wang, Xiaoyang  and
      Pan, Xiaoman  and
      Petzold, Linda  and
      Yu, Dong",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.268",
    doi = "10.18653/v1/2023.findings-acl.268",
    pages = "4381--4401",
    abstract = "Aspect or query-based summarization has recently caught more attention, as it can generate differentiated summaries based on users{'} interests. However, the current dataset for aspect or query-based summarization either focuses on specific domains, on a relatively small scale, or contains only a few aspect types. Such limitations hinder further explorations in this direction. In this work, we take advantage of crowd-sourcing knowledge on Wikipedia and automatically create a high-quality, large-scale open-domain aspect-based summarization dataset named OASum, which contains more than 3.7 million instances with around 1 million different aspects on 2 million Wikipedia pages. We provide benchmark results on OASum and demonstrate its ability for diverse aspect-based summarization generation. To overcome the data scarcity problem on specific domains, we also perform zero-shot, few-shot, and fine-tuning on seven downstream datasets. Specifically, zero/few-shot and fine-tuning results show that the model pre-trained on our corpus demonstrates a strong aspect or query-focused generation ability compared with the backbone model. Our dataset and pre-trained checkpoints are publicly available.",
}


@inproceedings{yang-etal-2023-shot,
    title = "Few-Shot Document-Level Event Argument Extraction",
    author = "Yang, Xianjun  and
      Lu, Yujie  and
      Petzold, Linda",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.446",
    doi = "10.18653/v1/2023.acl-long.446",
    pages = "8029--8046",
    abstract = "Event argument extraction (EAE) has been well studied at the sentence level but under-explored at the document level. In this paper, we study to capture event arguments that actually spread across sentences in documents. Prior works usually assume full access to rich document supervision, ignoring the fact that the available argument annotation is limited in production. To fill this gap, we present FewDocAE, a Few-Shot Document-Level Event Argument Extraction benchmark, based on the existing document-level event extraction dataset. We first define the new problem and reconstruct the corpus by a novel N-Way-D-Doc sampling instead of the traditional N-Way-K-Shot strategy. Then we adjust the current document-level neural models into the few-shot setting to provide baseline results under in- and cross-domain settings. Since the argument extraction depends on the context from multiple sentences and the learning process is limited to very few examples, we find this novel task to be very challenging with substantively low performance. Considering FewDocAE is closely related to practical use under low-resource regimes, we hope this benchmark encourages more research in this direction. Our data and codes will be available online.",
}


@inproceedings{yang2022pcmsp,
  title={PcMSP: A Dataset for Scientific Action Graphs Extraction from Polycrystalline Materials Synthesis Procedure Text},
  author={Yang, Xianjun and Zhuo, Ya and Zuo, Julia and Zhang, Xinlu and Wilson, Stephen and Petzold, Linda},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={6033--6046},
  year={2022}
}

@inproceedings{yang2021analysis,
  title={An Analysis of Relation Extraction within Sentences from Wet Lab Protocols},
  author={Yang, Xianjun and Zhang, Xinlu and Zuo, Julia and Wilson, Stephen and Petzold, Linda},
  booktitle={2021 IEEE International Conference on Big Data (Big Data)},
  pages={562--570},
  year={2021},
  organization={IEEE}
}

@article{lyu2021explosive,
  title={On explosive boiling of a multicomponent Leidenfrost drop},
  author={Lyu, Sijia and Tan, Huanshu and Wakata, Yuki and Yang, Xianjun and Law, Chung K and Lohse, Detlef and Sun, Chao},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={2},
  pages={e2016107118},
  year={2021},
  publisher={National Acad Sciences}
}

@article{jiang2019convective,
  title={Convective heat transfer along ratchet surfaces in vertical natural convection},
  author={Jiang, Hechuan and Zhu, Xiaojue and Mathai, Varghese and Yang, Xianjun and Verzicco, Roberto and Lohse, Detlef and Sun, Chao},
  journal={Journal of fluid mechanics},
  volume={873},
  pages={1055--1071},
  year={2019},
  publisher={Cambridge University Press}
}



